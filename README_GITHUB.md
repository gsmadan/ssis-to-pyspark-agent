# SSIS to PySpark Converter

ğŸš€ **Automated conversion tool for migrating SSIS packages to PySpark code with Databricks integration**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‹ Overview

This tool automates the conversion of SQL Server Integration Services (SSIS) packages to PySpark code optimized for Databricks. It uses a combination of rule-based mapping and AI-powered code refinement to ensure high-quality, production-ready PySpark code.

### Key Features

- âœ… **Automated SSIS Parsing**: Extracts control flow and data flow components from `.dtsx` files
- âœ… **Intelligent Mapping**: Rule-based conversion of SSIS components to PySpark equivalents
- âœ… **AI-Powered Refinement**: Optional LLM validation and code enhancement (OpenAI, Google Gemini, or Databricks)
- âœ… **Databricks Optimization**: Generated code is optimized for Databricks runtime
- âœ… **Schema Mapping**: Support for custom schema mapping from SSIS to Databricks catalogs
- âœ… **Comprehensive Reporting**: Detailed analysis and mapping statistics
- âœ… **Batch Processing**: Convert multiple packages at once

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SSIS Package   â”‚
â”‚   (.dtsx)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  XML Parser     â”‚ â”€â”€â–º Extracts connections, tasks, data flows
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Mapper    â”‚ â”€â”€â–º Maps SSIS components to PySpark
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Code Generator â”‚ â”€â”€â–º Generates PySpark code
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Validator  â”‚ â”€â”€â–º Refines and validates (optional)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PySpark Code   â”‚
â”‚   (.py)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- Git (for cloning the repository)

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/gsmadan/ssis-to-pyspark-converter.git
   cd ssis-to-pyspark-converter
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure API keys (optional, for AI features)**
   
   Create a `.env` file in the project root:
   ```bash
   # Choose one of the following providers:
   
   # Option 1: OpenAI
   DEFAULT_LLM_PROVIDER=openai
   OPENAI_API_KEY=your-openai-api-key
   
   # Option 2: Google Gemini
   DEFAULT_LLM_PROVIDER=gemini
   GEMINI_API_KEY=your-gemini-api-key
   
   # Option 3: Databricks Model Serving
   DEFAULT_LLM_PROVIDER=databricks
   DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
   DATABRICKS_TOKEN=your-databricks-token
   DATABRICKS_ENDPOINT=databricks-meta-llama-3-1-405b-instruct
   ```

## ğŸš€ Usage

### Basic Usage

**Convert a single SSIS package:**
```bash
python ssis_to_pyspark_app.py input/Sample_Package.dtsx
```

**Convert all packages in a folder:**
```bash
python ssis_to_pyspark_app.py input/
```

### Advanced Options

**Specify custom output directory:**
```bash
python ssis_to_pyspark_app.py input/package.dtsx --output custom_output/
```

**Use schema mapping:**
```bash
python ssis_to_pyspark_app.py input/package.dtsx --schema-mapping schema_mapping.json
```

**Skip AI validation (rule-based only):**
```bash
python ssis_to_pyspark_app.py input/package.dtsx --no-validation
```

**Enable verbose logging:**
```bash
python ssis_to_pyspark_app.py input/package.dtsx --verbose
```

### Schema Mapping

Create a `schema_mapping.json` file to map SSIS connections to Databricks schemas:

```json
{
  "connection_mappings": {
    "SourceDB": {
      "catalog": "dev_catalog",
      "schema": "bronze_layer",
      "description": "Source database connection"
    },
    "TargetDB": {
      "catalog": "dev_catalog",
      "schema": "silver_layer",
      "description": "Target database connection"
    }
  }
}
```

## ğŸ“Š Output Structure

After conversion, you'll find:

```
output/
â”œâ”€â”€ parsed_json/          # Intermediate JSON representation
â”‚   â””â”€â”€ Package_data_engineering.json
â”œâ”€â”€ pyspark_code/         # Generated PySpark code
â”‚   â””â”€â”€ Package_pyspark.py
â”œâ”€â”€ mapping_details/      # Mapping statistics
â”‚   â””â”€â”€ Package_mapping.json
â””â”€â”€ analysis/            # Conversion analysis report
    â””â”€â”€ conversion_report.json
```

## ğŸ¯ Supported SSIS Components

### Data Flow Components

| SSIS Component | PySpark Equivalent | Support Level |
|----------------|-------------------|---------------|
| OLE DB Source | `spark.table()` / `spark.sql()` | âœ… Full |
| OLE DB Destination | `df.write.saveAsTable()` | âœ… Full |
| Derived Column | `df.withColumn()` | âœ… Full |
| Conditional Split | `df.filter()` | âœ… Full |
| Lookup | `df.join()` | âœ… Full |
| Merge Join | `df.join()` | âœ… Full |
| Union All | `df.union()` | âœ… Full |
| Sort | `df.orderBy()` | âœ… Full |
| Aggregate | `df.groupBy().agg()` | âœ… Full |
| Data Conversion | `df.withColumn().cast()` | âœ… Full |
| Multicast | Multiple DataFrame assignments | âœ… Full |

### Control Flow Components

| SSIS Component | PySpark Equivalent | Support Level |
|----------------|-------------------|---------------|
| Execute SQL Task | `spark.sql()` | âœ… Full |
| Data Flow Task | PySpark DataFrame operations | âœ… Full |
| Sequence Container | Code blocks with comments | âœ… Full |
| For Loop Container | Python `for` loop | âš ï¸ Partial |
| Foreach Loop Container | Python `for` loop | âš ï¸ Partial |

## ğŸ§ª Running on Databricks

### Option 1: Upload as Notebook

1. Convert your SSIS package locally
2. Upload the generated `.py` file to Databricks workspace
3. Run as a Databricks notebook

### Option 2: Run as Job

1. Package the converter as a wheel or zip
2. Create a Databricks job
3. Attach the package and run

### Option 3: Use Databricks Volumes

```python
# In a Databricks notebook
import sys
sys.path.append("/Volumes/catalog/schema/ssis_converter/")

from ssis_to_pyspark_app import SSISToPySparkApp

app = SSISToPySparkApp(databricks_mode=True)
result = app.convert_single_package("/Volumes/catalog/schema/input/package.dtsx")
```

## ğŸ“ˆ Performance

- **Parsing**: ~2-5 seconds per package
- **Mapping**: ~1-3 seconds per package
- **AI Validation**: ~10-30 seconds per package (if enabled)

## ğŸ› ï¸ Development

### Project Structure

```
ssis-to-pyspark-agent/
â”œâ”€â”€ parsing/                    # SSIS XML parsing
â”‚   â””â”€â”€ data_engineering_parser.py
â”œâ”€â”€ mapping/                    # Component mapping logic
â”‚   â”œâ”€â”€ enhanced_json_mapper.py
â”‚   â”œâ”€â”€ schema_mapper.py
â”‚   â””â”€â”€ expression_translator.py
â”œâ”€â”€ code_generation/           # Code generation and validation
â”‚   â”œâ”€â”€ llm_code_validator.py
â”‚   â””â”€â”€ databricks_client.py
â”œâ”€â”€ ssis_to_pyspark_app.py    # Main application
â”œâ”€â”€ config.py                  # Configuration management
â”œâ”€â”€ models.py                  # Data models
â””â”€â”€ requirements.txt           # Dependencies
```

### Running Tests

```bash
pytest testing/
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built for enterprise SSIS to Databricks migrations
- Supports OpenAI GPT-4, Google Gemini, and Databricks Foundation Models
- Optimized for Databricks Unity Catalog

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**â­ If you find this tool useful, please consider giving it a star!**
